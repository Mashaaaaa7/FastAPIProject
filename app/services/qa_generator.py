from transformers import pipeline
import pdfplumber
import re
import unicodedata
from typing import List, Dict
import torch


class QAGenerator:
    def __init__(self, use_ollama: bool = False):
        self.device = 0 if torch.cuda.is_available() else -1
        self.use_ollama = use_ollama

        if use_ollama:
            print("‚è≥ –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama...")
            self.ollama_url = "http://localhost:11434/api/generate"
        else:
            print("‚è≥ –ó–∞–≥—Ä—É–∂–∞—é —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å...")
            self.generator = pipeline(
                "text2text-generation",
                model="cointegrated/rut5-base-multitask",
                device=self.device,
                torch_dtype=torch.float32
            )
            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        if not text:
            return ""
        text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C' or ch in '\n\t')
        text = re.sub(r'[>~<‚Ä¢¬ª¬´‚Äû"\[\]{}()_\-‚Äì‚Äî]+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def extract_key_entities(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏"""
        words = text.split()
        entities = []

        for word in words:
            clean_word = re.sub(r'[,.!?;:]+$', '', word)

            if (len(clean_word) > 4 and
                    clean_word[0].isupper() and
                    clean_word not in ['–í', '–ü–æ', '–û—Ç', '–ù–∞', '–°', '–ò', '–ß—Ç–æ']):
                entities.append(clean_word)

        return list(set(entities))[:5]

    def extract_summary(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        sentences = re.split(r'[.!?]+', text)
        long_sentences = [s.strip() for s in sentences if len(s.strip()) > 20]

        if long_sentences:
            return self.clean_text(long_sentences[0])
        return self.clean_text(text[:150])

    def extract_meaningful_text(self, file_path: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"""
        chunks = []
        try:
            with pdfplumber.open(file_path) as pdf:
                print(f"üìÑ PDF –∏–º–µ–µ—Ç {len(pdf.pages)} —Å—Ç—Ä–∞–Ω–∏—Ü")

                for i, page in enumerate(pdf.pages):
                    raw_text = page.extract_text()
                    if not raw_text:
                        continue

                    text = self.clean_text(raw_text)
                    if len(text) < 100:
                        continue

                    # –£–¥–∞–ª—è–µ–º —à—É–º–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã (–¥–∞—Ç—ã, –Ω–æ–º–µ—Ä–∞, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ)
                    text = re.sub(r'^\d{2}\.\d{2}\.\d{4}.*?Colab\s*', '', text)
                    text = re.sub(r'https?://[^\s]+', '', text)
                    text = re.sub(r'\d{4}.*?ipynb.*?Colab', '', text, flags=re.IGNORECASE)

                    paragraphs = [p.strip() for p in text.split('\n') if len(p.strip()) > 50]

                    for para in paragraphs:
                        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –°–ú–´–°–õ–û–í–´–ï –∫—É—Å–∫–∏ (–Ω–µ –ø—Ä–æ—Å—Ç–æ –ø–æ —Ç–æ—á–∫–∞–º)
                        chunks_from_para = self._split_into_chunks(para)
                        chunks.extend(chunks_from_para)

            # –§–∏–ª—å—Ç—Ä—É–µ–º –æ—Ç–∫—Ä–æ–≤–µ–Ω–Ω—ã–π –º—É—Å–æ—Ä
            chunks = [c for c in chunks if not any(
                bad in c['text'].lower() for bad in ['ipynb', 'colab', 'http', '¬©', '¬Æ']
            )]

            print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            return chunks
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return []

    def _split_into_chunks(self, text: str) -> List[Dict]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ –∫—É—Å–∫–∏"""
        chunks = []

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ç–æ—á–∫–∞–º, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª–∏–Ω—É
        sentences = re.split(r'[.!?]+\s+', text)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –æ–¥–Ω–æ
        combined = []
        current = ""

        for sent in sentences:
            sent = sent.strip()
            if not sent or len(sent) < 5:
                continue

            current += sent + ". "

            # –ï—Å–ª–∏ –Ω–∞–∫–æ–ø–∏–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º
            if len(current.split()) >= 12:  # –ú–∏–Ω–∏–º—É–º 12 —Å–ª–æ–≤
                combined.append(current.strip())
                current = ""

        if current.strip():
            combined.append(current.strip())

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ chunks
        for i, chunk_text in enumerate(combined):
            if len(chunk_text) > 50:  # –ú–∏–Ω–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤
                chunks.append({
                    "text": chunk_text,
                    "page": 0,  # –ù–µ –≤–∞–∂–Ω–æ –¥–ª—è –Ω–∞—Å
                    "word_count": len(chunk_text.split())
                })

        return chunks

    def generate_qa_pair_rut5(self, context: str) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ QA –ø–∞—Ä—ã"""
        try:
            context_clean = self.clean_text(context[:500])

            # –£–¥–∞–ª—è–µ–º —Ü–∏—Ñ—Ä—ã –∏ —à—É–º
            context_clean = re.sub(r'\b\d{1,3}\b', '', context_clean)
            context_clean = re.sub(r'\s+', ' ', context_clean).strip()

            if len(context_clean) < 60:
                return None

            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = [s.strip() for s in re.split(r'[.!?]+', context_clean)]
            long_sentences = [s for s in sentences if len(s.split()) >= 8]

            if not long_sentences:
                return None

            sentence = long_sentences[0]
            words = [w for w in sentence.split() if len(w) > 3]

            if len(words) < 5:
                return None

            # –ò—â–µ–º –†–ï–ê–õ–¨–ù–û–ï –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ (—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ)
            important_words = [
                w.lower() for w in words
                if w[0].isupper() and w.lower() not in
                   ['–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '—ç—Ç–æ', '—ç—Ç–∞']
            ]

            if not important_words:
                return None

            key_term = important_words[0]
            question = f"–û–±—ä—è—Å–Ω–∏—Ç–µ, —á—Ç–æ —Ç–∞–∫–æ–µ {key_term}?"
            answer = sentence

            if (len(question) > 15 and
                    len(answer) > 50 and
                    key_term in answer.lower()):
                return {
                    "question": question,
                    "answer": answer,
                    "context": context_clean[:150]
                }

            return None
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
            return None

    def create_quality_fallback(self, context: str) -> Dict:
        """Fallback ‚Äî –±–µ—Ä—ë–º —Ü–µ–ª–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫–∞–∫ –µ—Å—Ç—å"""
        try:
            context_clean = self.clean_text(context[:500])

            # –ò—â–µ–º —Ö–æ—Ä–æ—à–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            sentences = [s.strip() for s in re.split(r'[.!?]+', context_clean)
                         if len(s.strip()) > 50]

            if not sentences:
                return None

            sentence = sentences[0]
            words = sentence.split()

            if len(words) < 7:
                return None

            # –ë–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ-–≤—Ç–æ—Ä–æ–µ —Å–ª–æ–≤–æ –∫–∞–∫ —Ç–µ–º—É
            topic = ' '.join(words[:2]).lower()

            question = f"–û–±—ä—è—Å–Ω–∏—Ç–µ, —á—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥—ë—Ç, –µ—Å–ª–∏ {topic}?"
            answer = sentence

            if len(answer) > 40:
                return {
                    "question": question,
                    "answer": answer,
                    "context": context_clean[:120]
                }

            return None
        except:
            return None

    def process_pdf(self, file_path: str, max_cards: int = 10) -> List[Dict]:
        print(f"\nüîÑ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {file_path}...")
        print(f"üéØ –¶–µ–ª—å: {max_cards} –∫–∞—Ä—Ç–æ—á–µ–∫")

        chunks = self.extract_meaningful_text(file_path)

        if not chunks:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤!")
            return []

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

        chunks.sort(key=lambda x: abs(x['word_count'] - 25))
        flashcards = []

        for chunk in chunks[:max_cards * 2]:
            if len(flashcards) >= max_cards:
                break

            qa_pair = self.generate_qa_pair_rut5(chunk['text'])

            if qa_pair:
                flashcard = {
                    "id": len(flashcards) + 1,
                    "question": qa_pair["question"],
                    "answer": qa_pair["answer"],
                    "context": qa_pair["context"],
                    "source": f"Page {chunk['page']}"
                }
                flashcards.append(flashcard)
                print(f"  ‚úÖ [{len(flashcards)}] {qa_pair['question'][:60]}...")
            else:
                fallback_qa = self.create_quality_fallback(chunk['text'])
                flashcard = {
                    "id": len(flashcards) + 1,
                    "question": fallback_qa["question"],
                    "answer": fallback_qa["answer"],
                    "context": fallback_qa["context"],
                    "source": f"Page {chunk['page']}"
                }
                flashcards.append(flashcard)
                print(f"  üîÑ [{len(flashcards)}] {fallback_qa['question'][:60]}...")

        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(flashcards)} –∫–∞—Ä—Ç–æ—á–µ–∫")
        return flashcards