from transformers import pipeline
import pdfplumber
import re
import unicodedata
from typing import List, Dict
import torch


class QAGenerator:
    def __init__(self):
        self.device = 0 if torch.cuda.is_available() else -1
        print("‚è≥ –ó–∞–≥—Ä—É–∂–∞—é —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å...")
        self.generator = pipeline(
            "text2text-generation",
            model="cointegrated/rut5-base-multitask",
            device=self.device,
            torch_dtype=torch.float32
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        if not text:
            return ""
        text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C' or ch in '\n\t')
        text = re.sub(r'[>~<‚Ä¢¬ª¬´‚Äû"\[\]{}()_\-‚Äì‚Äî]+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def extract_meaningful_text(self, file_path: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"""
        chunks = []
        try:
            with pdfplumber.open(file_path) as pdf:
                print(f"üìÑ PDF –∏–º–µ–µ—Ç {len(pdf.pages)} —Å—Ç—Ä–∞–Ω–∏—Ü")

                for i, page in enumerate(pdf.pages):
                    raw_text = page.extract_text()
                    if not raw_text:
                        continue

                    text = self.clean_text(raw_text)
                    if len(text) < 100:
                        continue

                    text = re.sub(r'^\d{2}\.\d{2}\.\d{4}.*?Colab\s*', '', text)
                    text = re.sub(r'https?://[^\s]+', '', text)
                    text = re.sub(r'\d{4}.*?ipynb.*?Colab', '', text, flags=re.IGNORECASE)

                    paragraphs = [p.strip() for p in text.split('\n') if len(p.strip()) > 50]

                    for para in paragraphs:
                        chunks_from_para = self._split_into_chunks(para)
                        chunks.extend(chunks_from_para)

            chunks = [c for c in chunks if not any(
                bad in c['text'].lower() for bad in ['ipynb', 'colab', 'http', '¬©', '¬Æ']
            )]

            print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            return chunks
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return []

    def _split_into_chunks(self, text: str) -> List[Dict]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ –∫—É—Å–∫–∏"""
        chunks = []
        sentences = re.split(r'[.!?]+\s+', text)

        combined = []
        current = ""

        for sent in sentences:
            sent = sent.strip()
            if not sent or len(sent) < 5:
                continue

            current += sent + ". "

            if len(current.split()) >= 12:
                combined.append(current.strip())
                current = ""

        if current.strip():
            combined.append(current.strip())

        for chunk_text in combined:
            if len(chunk_text) > 60:
                chunks.append({
                    "text": chunk_text,
                    "page": 0,
                    "word_count": len(chunk_text.split())
                })

        return chunks

    def _extract_key_phrase(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª–∞–≤–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        words = text.split()

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ
        bad_words = {
            '—ç—Ç–æ', '–¥–ª—è', '–ø—Ä–∏', '–∫–∞–∫', '—á—Ç–æ', '–≤', '–ø–æ', '–Ω–∞', '—Å', '–∏', '–∏–ª–∏', '—Ç–æ',
            '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–∏', '–±—ã—Ç—å', '—è–≤–ª—è—é—Ç—Å—è', '—è–≤–ª—è–µ—Ç—Å—è', '–µ—Å—Ç—å',
            '–µ—Å–ª–∏', '–∑–¥–µ—Å—å', '–Ω–∞–∫–æ–Ω–µ—Ü', '–æ–¥–Ω–∞–∫–æ', '—Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—è', '–≤—ã–¥–µ–ª–µ–Ω–∏–µ',
            '–ø—Ä–µ–¥–∏—Å–ª–æ–≤–∏–µ', '–æ–±—É—á–µ–Ω–∏–µ', '–Ω–∞–±–æ—Ä'
        }

        idx = 0
        while idx < len(words) and words[idx].lower() in bad_words:
            idx += 1

        working_words = words[idx:]

        # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ (—Å–ª–æ–≤–æ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã, –¥–ª–∏–Ω–∞ > 5)
        for w in working_words[:10]:
            w_lower = w.lower().rstrip(',:;.')
            if len(w_lower) > 5 and w[0].isupper() and w_lower not in bad_words:
                return w_lower

        # Fallback
        return "–∫–æ–Ω—Ü–µ–ø—Ü–∏—è"

    def _generate_question(self, text: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –£–ú–ù–´–ô –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()
        key_phrase = self._extract_key_phrase(text)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π –≤–æ–ø—Ä–æ—Å

        if any(word in text_lower for word in ['–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å', '–≤–≤–æ–¥–∏—Ç', '—Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω—ã']):
            return f"–ß—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π {key_phrase}?"

        elif any(word in text_lower for word in ['–æ–±—É—á–µ–Ω–∏—è', '–∞–ª–≥–æ—Ä–∏—Ç–º', '–º–µ—Ç–æ–¥—ã', '–ø–æ–¥—Ö–æ–¥']):
            return f"–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç {key_phrase}?"

        elif any(word in text_lower for word in ['–ø—Ä–∏–º–µ–Ω–µ–Ω–∏', '–∏—Å–ø–æ–ª—å–∑–æ–≤–∞', '–ø—Ä–∏–º–µ–Ω—è–µ—Ç']):
            return f"–ì–¥–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è {key_phrase}?"

        elif any(word in text_lower for word in ['—Ä–∞—Å—Å–º–æ—Ç—Ä', '–æ–±—Å—É–∂–¥–∞', '–∞–Ω–∞–ª–∏–∑–∏—Ä']):
            return f"–ö–∞–∫–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏–º–µ–µ—Ç {key_phrase}?"

        elif any(word in text_lower for word in ['—Å–æ–¥–µ—Ä–∂–∏—Ç', '–≤–∫–ª—é—á–∞–µ—Ç', '—Å–æ—Å—Ç–æ–∏—Ç']):
            return f"–ò–∑ —á–µ–≥–æ —Å–æ—Å—Ç–æ–∏—Ç {key_phrase}?"

        elif any(word in text_lower for word in ['–º–æ–∂–µ—Ç', '–ø–æ–º–æ–≥–∞–µ—Ç', '—Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç']):
            return f"–ö–∞–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è —É {key_phrase}?"

        elif any(word in text_lower for word in ['–¥–∞–Ω–Ω—ã–µ', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã']):
            return f"–ö–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å {key_phrase}?"

        elif any(word in text_lower for word in ['–ø—Ä–æ—Ü–µ—Å—Å', '—ç—Ç–∞–ø—ã', '—à–∞–≥–∏']):
            return f"–ö–∞–∫–∏–µ —ç—Ç–∞–ø—ã —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ—Ü–µ—Å—Å {key_phrase}?"

        else:
            return f"–û–±—ä—è—Å–Ω–∏—Ç–µ, —á—Ç–æ —Ç–∞–∫–æ–µ {key_phrase}?"

    def generate_qa_pair(self, context: str) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é QA –ø–∞—Ä—É"""
        try:
            context_clean = self.clean_text(context[:700])
            context_clean = re.sub(r'\b\d{1,3}\b', '', context_clean)
            context_clean = re.sub(r'\s+', ' ', context_clean).strip()

            if len(context_clean) < 100:
                return None

            if any(word in context_clean.lower() for word in
                   ['–∫–æ–¥', 'import', 'def ', 'print(', 'function', 'class ']):
                return None

            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = [s.strip() for s in re.split(r'[.!?]+', context_clean)]
            candidate_sents = [s for s in sentences if len(s.split()) >= 12 and len(s) > 90]

            if not candidate_sents:
                return None

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–≤–æ–¥–Ω—ã–µ —Ñ—Ä–∞–∑—ã
            answer = None
            for sent in candidate_sents:
                if not any(marker in sent.lower() for marker in
                           ['–Ω–æ–º–µ—Ä', '—Ç–µ–º–∞', '—Ä–∞–∑–¥–µ–ª', '–≥–ª–∞–≤–∞', '—Ç–∞–±–ª–∏—Ü–∞', '—Ä–∏—Å—É–Ω–æ–∫']):
                    answer = sent
                    break

            if not answer:
                answer = candidate_sents[0]

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å
            question = self._generate_question(answer)

            answer = re.sub(r'\s+', ' ', answer).strip()
            question = re.sub(r'\s+', ' ', question).strip()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            if (len(question) > 12 and len(answer) > 90 and
                    '?' in question and
                    len(question) < 120):
                return {
                    "question": question,
                    "answer": answer,
                    "context": context_clean[:150]
                }

            return None

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return None

    def process_pdf(self, file_path: str, max_cards: int = 10) -> List[Dict]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç PDF –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞—Ä—Ç–æ—á–∫–∏"""
        print(f"\nüîÑ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {file_path}...")
        print(f"üéØ –¶–µ–ª—å: {max_cards} –∫–∞—Ä—Ç–æ—á–µ–∫")

        chunks = self.extract_meaningful_text(file_path)

        if not chunks:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤!")
            return []

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

        chunks.sort(key=lambda x: abs(x['word_count'] - 25))
        flashcards = []

        for chunk in chunks[:max_cards * 2]:
            if len(flashcards) >= max_cards:
                break

            qa_pair = self.generate_qa_pair(chunk['text'])

            if qa_pair:
                flashcard = {
                    "id": len(flashcards) + 1,
                    "question": qa_pair["question"],
                    "answer": qa_pair["answer"],
                    "context": qa_pair["context"],
                    "source": f"Page {chunk['page']}"
                }
                flashcards.append(flashcard)
                print(f"  ‚úÖ [{len(flashcards)}] {qa_pair['question'][:60]}...")

        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(flashcards)} –∫–∞—Ä—Ç–æ—á–µ–∫")
        return flashcards
